

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Attention Bridge &mdash; MAMMOTH  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]]}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="install.html" />
    <link rel="prev" title="Config-config Tool" href="config_config.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MAMMOTH
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ref.html">References</a></li>
</ul>
<p class="caption"><span class="caption-text">Frequently Asked Questions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">About MAMMOTH</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#mammoth-and-opennmt">MAMMOTH and OpenNMT</a></li>
</ul>
<p class="caption"><span class="caption-text">MAMMOTH features</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="modular_model.html">Component-level Modularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_config.html">Config-config Tool</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Attention Bridge</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linattentionbridgelayer">LinAttentionBridgeLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#perceiverattentionbridgelayer">PerceiverAttentionBridgeLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#simpleattentionbridgelayer">SimpleAttentionBridgeLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transformerattentionbridgelayer">TransformerAttentionBridgeLayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#feedforwardattentionbridgelayer">FeedForwardAttentionBridgeLayer</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="prepare_data.html">Prepare Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/train_mammoth_101.html">Training MAMMOTH 101</a></li>
</ul>
<p class="caption"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="options/build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/translate.html">Translate</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/server.html">Server</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mammoth.html">Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.inputters.html">Data Loaders</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MAMMOTH</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Attention Bridge</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/attention_bridges.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="attention-bridge">
<h1>Attention Bridge<a class="headerlink" href="#attention-bridge" title="Permalink to this headline">¶</a></h1>
<p>The embeddings are generated through the self-attention mechanism of the encoder and establish a connection with language-specific decoders that focus their attention on these embeddings. This is why they are referred to as ‘bridges’. This architectural element serves to link the encoded information with the decoding process, enhancing the flow of information between different stages of language processing.</p>
<p>There are five types of attention mechanism implemented:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer_type_to_cls</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;lin&#39;</span><span class="p">:</span> <span class="n">LinAttentionBridgeLayer</span><span class="p">,</span>
            <span class="s1">&#39;perceiver&#39;</span><span class="p">:</span> <span class="n">PerceiverAttentionBridgeLayer</span><span class="p">,</span>
            <span class="s1">&#39;simple&#39;</span><span class="p">:</span> <span class="n">SimpleAttentionBridgeLayer</span><span class="p">,</span>
            <span class="s1">&#39;transformer&#39;</span><span class="p">:</span> <span class="n">TransformerAttentionBridgeLayer</span><span class="p">,</span>
            <span class="s1">&#39;feedforward&#39;</span><span class="p">:</span> <span class="n">FeedForwardAttentionBridgeLayer</span><span class="p">,</span>
        <span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">intermediate_output</span></code> refers to the input of the attention bridge, specifically denoting the output that emerges from the encoder, i.e., \( \mathbf{H}\in \mathbf{R}^{n\times d_h} \) <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq_len,</span> <span class="pre">hidden_dim)</span></code>. In other words, this is the transformed representation of the input data after it has undergone encoding processes. This intermediate output serves as the foundation for the subsequent attention bridge, facilitating the connection between the encoded information and the subsequent stages of language processing.</p>
<div class="section" id="linattentionbridgelayer">
<h2>LinAttentionBridgeLayer<a class="headerlink" href="#linattentionbridgelayer" title="Permalink to this headline">¶</a></h2>
<p>The attention bridge employed is based on the structured self-attention mechanism introduced by Lin et al. in their 2017 paper, accessible at <a class="reference external" href="https://arxiv.org/abs/1703.03130">https://arxiv.org/abs/1703.03130</a>. This mechanism is utilized to establish the connection between different components of the architecture.
This attention bridge allows the encoded information to be channeled effectively into subsequent language-specific decoding processes, contributing to the overall performance of the architecture.</p>
</div>
<div class="section" id="perceiverattentionbridgelayer">
<h2>PerceiverAttentionBridgeLayer<a class="headerlink" href="#perceiverattentionbridgelayer" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">PerceiverAttentionBridgeLayer</span></code> involves a multi-headed dot product self-attention mechanism, where the type of attention (<code class="docutils literal notranslate"><span class="pre">att_type</span></code>) can take on one of two values: ‘context’ or ‘self’. This mechanism is structured as follows:</p>
<ol class="simple">
<li><p><strong>MultiHeadedAttention</strong>: This module performs multi-headed dot product self-attention. It takes the input data and applies self-attention with multiple heads.</p></li>
<li><p><strong>AttentionBridgeNorm</strong>: The output of the multi-headed attention mechanism is passed through a normalization process that helps ensure stable learning..</p></li>
<li><p><strong>Linear Layer</strong>: After normalization, the data is fed into a linear layer. This linear transformation can be seen as a learned projection of the attention-weighted data into a new space.</p></li>
<li><p><strong>ReLU Activation</strong>: The output of the linear layer undergoes the Rectified Linear Unit (ReLU) activation function.</p></li>
<li><p><strong>Linear Layer (Second)</strong>: Another linear layer is applied to the ReLU-activated output.</p></li>
<li><p><strong>AttentionBridgeNorm (Second)</strong>: Similar to the earlier normalization step, the output of the second linear layer is normalized using the AttentionBridgeNorm module.</p></li>
</ol>
</div>
<div class="section" id="simpleattentionbridgelayer">
<h2>SimpleAttentionBridgeLayer<a class="headerlink" href="#simpleattentionbridgelayer" title="Permalink to this headline">¶</a></h2>
<p>The process described involves dot product self-attention. The steps are as follows:</p>
<ol class="simple">
<li><p><strong>Input Transformation</strong>: Given an input matrix \(\mathbf{H} \in \mathbb{R}^{d_h \times n}\), two sets of learned weight matrices are used to transform the input. These weight matrices are \( \mathbf{W}_1 \in \mathbb{R}^{d_h \times d_a}\) and \( \mathbf{W}_2 \in \mathbb{R}^{d_h \times d_a}\). The multiplication of \(\mathbf{H}\) with \(\mathbf{W}_1\) and \( \mathbf{W}_2\) produces matrices \( \mathbf{V} \) and \( \mathbf{K}\), respectively:</p>
<ul class="simple">
<li><p>\( \mathbf{V} = \mathbf{H} \mathbf{W}_1\)</p></li>
<li><p>\( \mathbf{K} = \mathbf{H} \mathbf{W}_2\)</p></li>
</ul>
</li>
<li><p><strong>Attention Calculation</strong>: The core attention calculation involves three matrices: \( \mathbf{Q} \in \mathbb{R}^{d_h \times n}\), \( \mathbf{K} \) (calculated previously), and \( \mathbf{V}\) (calculated previously). The dot product of \( \mathbf{Q}\) and \(\mathbf{K}^\top\) is divided by the square root of the dimensionality of the input features (\( \sqrt{d_h}\)).
The final attended output is calculated by multiplying the attention weights with the \( \mathbf{V} \) matrix: \( \mathbf{H}^\prime = \operatorname{Softmax}(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_h}})\mathbf{V}\)</p></li>
</ol>
</div>
<div class="section" id="transformerattentionbridgelayer">
<h2>TransformerAttentionBridgeLayer<a class="headerlink" href="#transformerattentionbridgelayer" title="Permalink to this headline">¶</a></h2>
<p>The TransformerEncoderLayer employs multi-headed dot product self-attention (by  <code class="docutils literal notranslate"><span class="pre">TransformerEncoderLayer</span></code>) to capture relationships within the input sequence.</p>
</div>
<div class="section" id="feedforwardattentionbridgelayer">
<h2>FeedForwardAttentionBridgeLayer<a class="headerlink" href="#feedforwardattentionbridgelayer" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">FeedForwardAttentionBridgeLayer</span></code> module applies a sequence of linear transformations and <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> activations to the input data, followed by an attention bridge normalization, enhancing the connectivity between different parts of the model.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="install.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="config_config.html" class="btn btn-neutral float-left" title="Config-config Tool" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, HelsinkiNLP

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>