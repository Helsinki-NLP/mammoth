# This configuration showcases some of the features of Mammoth and config-config
config_config:
  src_path: "data/opus/{sorted_pair}/opus.{sorted_pair}-train.{src_lang}"
  tgt_path: "data/opus/{sorted_pair}/opus.{sorted_pair}-train.{tgt_lang}"
  valid_src_path: "data/opus/{sorted_pair}/opus.{sorted_pair}-valid.{src_lang}"
  valid_tgt_path: "data/opus/{sorted_pair}/opus.{sorted_pair}-valid.{tgt_lang}"
  # This uses parallel data paired with the English language as monolingual data.
  # Only existing files are added: one of the patterns will match and the other will not, depending on the language
  ae_path:
    - "data/opus/en-{tgt_lang}/opus.en-{tgt_lang}-train.{tgt_lang}"
    - "data/opus/{tgt_lang}-en/opus.{tgt_lang}-en-train.{tgt_lang}"
  autoencoder: True
  # distance_matrix not set here, but could use examples/config_config.distance.csv
  distance_matrix: null
  n_groups: 6
  use_weight: True
  temperature: 0.5
  use_introduce_at_training_step: True
  zero_shot: True
  transforms:
    - sentencepiece
    - filtertoolong
  ae_transforms:
    - sentencepiece
    - filtertoolong
    - denoising
  enc_sharing_groups:
    - GROUP
    - FULL
  dec_sharing_groups:
    - GROUP
    - FULL
    - GROUP
  n_gpus_per_node: 4
  n_nodes: 2
  use_src_lang_token: False

  # Note that this specifies the groups manually instead of clustering
  groups:
    "en": "en"
    "af": "af+nl"
    "da": "da+sv"
    "es": "es+it"
    "et": "et+fi"
    "fi": "et+fi"
    "it": "es+it"
    "nl": "af+nl"
    "sv": "da+sv"


# vocabs serve two purposes: defines the vocab files, and gives the potential languages to consider
src_vocab:
  "af": "data/opus/vocabs/opusTC.afr.32k.spm.vocab"
  "da": "data/opus/vocabs/opusTC.dan.32k.spm.vocab"
  "en": "data/opus/vocabs/opusTC.eng.32k.spm.vocab"
  "es": "data/opus/vocabs/opusTC.spa.32k.spm.vocab"
  "et": "data/opus/vocabs/opusTC.est.32k.spm.vocab"
  "fi": "data/opus/vocabs/opusTC.fin.32k.spm.vocab"
  "it": "data/opus/vocabs/opusTC.ita.32k.spm.vocab"
  "nl": "data/opus/vocabs/opusTC.nld.32k.spm.vocab"
  "sv": "data/opus/vocabs/opusTC.swe.32k.spm.vocab"
tgt_vocab:
  "af": "data/opus/vocabs/opusTC.afr.32k.spm.vocab"
  "da": "data/opus/vocabs/opusTC.dan.32k.spm.vocab"
  "en": "data/opus/vocabs/opusTC.eng.32k.spm.vocab"
  "es": "data/opus/vocabs/opusTC.spa.32k.spm.vocab"
  "et": "data/opus/vocabs/opusTC.est.32k.spm.vocab"
  "fi": "data/opus/vocabs/opusTC.fin.32k.spm.vocab"
  "it": "data/opus/vocabs/opusTC.ita.32k.spm.vocab"
  "nl": "data/opus/vocabs/opusTC.nld.32k.spm.vocab"
  "sv": "data/opus/vocabs/opusTC.swe.32k.spm.vocab"

adapters:
  encoder:
    enc_lang_bottom:
      adapter_type: lora
      layer_stack_index: 0
      layers: [0, 1, 2]
      hidden_dim: 8
      ids: LANGUAGE
    enc_lang_top:
      adapter_type: lora
      layer_stack_index: 1
      layers: [0, 1, 2]
      hidden_dim: 8
      ids: LANGUAGE
  decoder:
    dec_lang_bottom:
      adapter_type: ff
      layer_stack_index: 0
      layers: [0, 1]
      hidden_dim: 16
      ids: LANGUAGE
    dec_lang_mid:
      adapter_type: ff
      layer_stack_index: 1
      layers: [0, 1, 2]
      hidden_dim: 16
      ids: LANGUAGE
    dec_lang_top:
      adapter_type: ff
      layer_stack_index: 2
      layers: [0]
      hidden_dim: 16
      ids: LANGUAGE

save_model: models/opus.spm32k.adafactor.hamburger.l2.dsae/opus.spm32k.adafactor.hamburger.l2.dsae

batch_size: 32768
batch_type: tokens
normalization: tokens
valid_batch_size: 4096
model_dim: 512
enc_layers: [3, 3]
dec_layers: [2, 3, 1]
dropout: 0.1
label_smoothing: 0.1
weight_decay: 0.05
param_init: 0.0
param_init_glorot: true
train_steps: 150000
valid_steps: 1000
warmup_steps: 5000
report_every: 100
save_checkpoint_steps: 10000
keep_checkpoint: 3
accum_count: 8
optim: adafactor
decay_method: linear_warmup
learning_rate: 0.00003
max_grad_norm: 1.0
seed: 3435
model_type: text
#### Sentencepiece
src_subword_type: sentencepiece
trg_subword_type: sentencepiece
src_subword_nbest: 5
tgt_subword_nbest: 5
# Note that the variables here are populated by OpenNMT code, not config_config
src_subword_model: models/tatoeba_spm/opusTC.{src_lang}.32k.spm
tgt_subword_model: models/tatoeba_spm/opusTC.{tgt_lang}.32k.spm
#### Filter
src_seq_length: 200
tgt_seq_length: 200
#### Bart
denoising_objective: bart
mask_length: span-poisson
poisson_lambda: 3.0
mask_ratio: 0.2
replace_length: 1

x_transformers_opts:
  attn_flash: True
  heads: 16
  rotary_pos_emb: True
  tie_embedding: True
