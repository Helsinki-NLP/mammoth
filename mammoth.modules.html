

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Modules &mdash; MAMMOTH  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]]}})</script>
        <script src="https://unpkg.com/mermaid@8.4.8/dist/mermaid.min.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Translation" href="mammoth.translation.html" />
    <link rel="prev" title="Framework" href="mammoth.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MAMMOTH
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ref.html">References</a></li>
</ul>
<p class="caption"><span class="caption-text">MAMMOTH features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="config_config.html">Config-config tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention_bridges.html">Attention Bridge</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="prepare_data.html">Prepare Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/Translation.html">Translation</a></li>
</ul>
<p class="caption"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="options/build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/translate.html">Translate</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/server.html">Server</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mammoth.html">Framework</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#embeddings">Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#attention-bridge">Attention Bridge</a></li>
<li class="toctree-l2"><a class="reference internal" href="#encoders">Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#decoders">Decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sublayers">Sublayers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="mammoth.inputters.html">Data Loaders</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MAMMOTH</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Modules</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/mammoth.modules.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="modules">
<h1>Modules<a class="headerlink" href="#modules" title="Permalink to this headline">¶</a></h1>
<div class="section" id="embeddings">
<h2>Embeddings<a class="headerlink" href="#embeddings" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mammoth.modules.Embeddings">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.</code><code class="sig-name descname">Embeddings</code><span class="sig-paren">(</span><em class="sig-param">word_vec_size</em>, <em class="sig-param">word_vocab_size</em>, <em class="sig-param">word_padding_idx</em>, <em class="sig-param">position_encoding=False</em>, <em class="sig-param">feat_merge='concat'</em>, <em class="sig-param">feat_vec_exponent=0.7</em>, <em class="sig-param">feat_vec_size=-1</em>, <em class="sig-param">feat_padding_idx=[]</em>, <em class="sig-param">feat_vocab_sizes=[]</em>, <em class="sig-param">dropout=0</em>, <em class="sig-param">freeze_word_vecs=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/embeddings.html#Embeddings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.Embeddings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Words embeddings for encoder/decoder.</p>
<p>Additionally includes ability to add input features
based on “Linguistic Input Features Improve Neural Machine Translation”
<a class="bibtex reference internal" href="ref.html#sennrich2016linguistic" id="id1">[SH16]</a>.</p>
<script>mermaid.initialize({startOnLoad:true});</script><div class="mermaid">
            graph LR
   A[Input]
   C[Feature 1 Lookup]
   A--&gt;B[Word Lookup]
   A--&gt;C
   A--&gt;D[Feature N Lookup]
   B--&gt;E[MLP/Concat]
   C--&gt;E
   D--&gt;E
   E--&gt;F[Output]
        </div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>word_vec_size</strong> (<em>int</em>) – size of the dictionary of embeddings.</p></li>
<li><p><strong>word_padding_idx</strong> (<em>int</em>) – padding index for words in the embeddings.</p></li>
<li><p><strong>feat_padding_idx</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – padding index for a list of features
in the embeddings.</p></li>
<li><p><strong>word_vocab_size</strong> (<em>int</em>) – size of dictionary of embeddings for words.</p></li>
<li><p><strong>feat_vocab_sizes</strong> (<em>List</em><em>[</em><em>int</em><em>]</em><em>, </em><em>optional</em>) – list of size of dictionary
of embeddings for each feature.</p></li>
<li><p><strong>position_encoding</strong> (<em>bool</em>) – see <code class="xref py py-class docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></p></li>
<li><p><strong>feat_merge</strong> (<em>string</em>) – merge action for the features embeddings:
concat, sum or mlp.</p></li>
<li><p><strong>feat_vec_exponent</strong> (<em>float</em>) – when using <cite>-feat_merge concat</cite>, feature
embedding size is N^feat_dim_exponent, where N is the
number of values the feature takes.</p></li>
<li><p><strong>feat_vec_size</strong> (<em>int</em>) – embedding dimension for features when using
<cite>-feat_merge mlp</cite></p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout probability.</p></li>
<li><p><strong>freeze_word_vecs</strong> (<em>bool</em>) – freeze weights of word vectors.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.Embeddings.emb_luts">
<em class="property">property </em><code class="sig-name descname">emb_luts</code><a class="headerlink" href="#mammoth.modules.Embeddings.emb_luts" title="Permalink to this definition">¶</a></dt>
<dd><p>Embedding look-up table.</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.Embeddings.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">source</em>, <em class="sig-param">step=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/embeddings.html#Embeddings.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.Embeddings.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the embeddings for words and features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>source</strong> (<em>LongTensor</em>) – index tensor <code class="docutils literal notranslate"><span class="pre">(len,</span> <span class="pre">batch,</span> <span class="pre">nfeat)</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Word embeddings <code class="docutils literal notranslate"><span class="pre">(len,</span> <span class="pre">batch,</span> <span class="pre">embedding_size)</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>FloatTensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.Embeddings.load_pretrained_vectors">
<code class="sig-name descname">load_pretrained_vectors</code><span class="sig-paren">(</span><em class="sig-param">emb_file</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/embeddings.html#Embeddings.load_pretrained_vectors"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.Embeddings.load_pretrained_vectors" title="Permalink to this definition">¶</a></dt>
<dd><p>Load in pretrained embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>emb_file</strong> (<em>str</em>) – path to torch serialized embeddings</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.Embeddings.word_lut">
<em class="property">property </em><code class="sig-name descname">word_lut</code><a class="headerlink" href="#mammoth.modules.Embeddings.word_lut" title="Permalink to this definition">¶</a></dt>
<dd><p>Word look-up table.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="attention-bridge">
<h2>Attention Bridge<a class="headerlink" href="#attention-bridge" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mammoth.modules.attention_bridge.AttentionBridge">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.attention_bridge.</code><code class="sig-name descname">AttentionBridge</code><span class="sig-paren">(</span><em class="sig-param">layers</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/attention_bridge.html#AttentionBridge"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.attention_bridge.AttentionBridge" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>N-layered attention-bridge between encoders-&gt;decoders</p>
<dl class="method">
<dt id="mammoth.modules.attention_bridge.AttentionBridge.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">enc_output</em>, <em class="sig-param">mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/attention_bridge.html#AttentionBridge.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.attention_bridge.AttentionBridge.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass for the bridge layers</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.attention_bridge.AttentionBridge.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opts</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/attention_bridge.html#AttentionBridge.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.attention_bridge.AttentionBridge.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="encoders">
<h2>Encoders<a class="headerlink" href="#encoders" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mammoth.modules.encoder.EncoderBase">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.encoder.</code><code class="sig-name descname">EncoderBase</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/encoder.html#EncoderBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.encoder.EncoderBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Base encoder class. Specifies the interface used by different encoder types
and required by <code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.Models.NMTModel</span></code>.</p>
<div class="mermaid">
            graph BT
   A[Input]
   subgraph RNN
     C[Pos 1]
     D[Pos 2]
     E[Pos N]
   end
   F[Memory_Bank]
   G[Final]
   A--&gt;C
   A--&gt;D
   A--&gt;E
   C--&gt;F
   D--&gt;F
   E--&gt;F
   E--&gt;G
        </div><dl class="method">
<dt id="mammoth.modules.encoder.EncoderBase.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/encoder.html#EncoderBase.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.encoder.EncoderBase.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>LongTensor</em>) – padded sequences of sparse indices <code class="docutils literal notranslate"><span class="pre">(src_len,</span> <span class="pre">batch,</span> <span class="pre">nfeat)</span></code></p></li>
<li><p><strong>lengths</strong> (<em>LongTensor</em>) – length of each sequence <code class="docutils literal notranslate"><span class="pre">(batch,)</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>final encoder state, used to initialize decoder</p></li>
<li><p>memory bank for attention, <code class="docutils literal notranslate"><span class="pre">(src_len,</span> <span class="pre">batch,</span> <span class="pre">hidden)</span></code></p></li>
<li><p>lengths</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor, FloatTensor, FloatTensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.transformer_encoder.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.transformer_encoder.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">num_layers</em>, <em class="sig-param">d_model</em>, <em class="sig-param">heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout</em>, <em class="sig-param">attention_dropout</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">max_relative_positions</em>, <em class="sig-param">pos_ffn_activation_fn='relu'</em>, <em class="sig-param">layer_norm_module=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/transformer_encoder.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.transformer_encoder.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mammoth.modules.encoder.EncoderBase" title="mammoth.modules.encoder.EncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.modules.encoder.EncoderBase</span></code></a></p>
<p>The Transformer encoder from “Attention is All You Need”
<a class="bibtex reference internal" href="ref.html#dblp-journals-corr-vaswanispujgkp17" id="id2">[VSP+17]</a></p>
<div class="mermaid">
            graph BT
   A[input]
   B[multi-head self-attn]
   C[feed forward]
   O[output]
   A --&gt; B
   B --&gt; C
   C --&gt; O
        </div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_layers</strong> (<em>int</em>) – number of encoder layers</p></li>
<li><p><strong>d_model</strong> (<em>int</em>) – size of the model</p></li>
<li><p><strong>heads</strong> (<em>int</em>) – number of heads</p></li>
<li><p><strong>d_ff</strong> (<em>int</em>) – size of the inner FF layer</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout parameters</p></li>
<li><p><strong>embeddings</strong> (<a class="reference internal" href="#mammoth.modules.Embeddings" title="mammoth.modules.Embeddings"><em>mammoth.modules.Embeddings</em></a>) – embeddings to use, should have positional encodings</p></li>
<li><p><strong>pos_ffn_activation_fn</strong> (<em>ActivationFunction</em>) – activation function choice for PositionwiseFeedForward layer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embeddings <code class="docutils literal notranslate"><span class="pre">(src_len,</span> <span class="pre">batch_size,</span> <span class="pre">model_dim)</span></code></p></li>
<li><p>memory_bank <code class="docutils literal notranslate"><span class="pre">(src_len,</span> <span class="pre">batch_size,</span> <span class="pre">model_dim)</span></code></p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(torch.FloatTensor, torch.FloatTensor)</p>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.transformer_encoder.TransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">lengths=None</em>, <em class="sig-param">skip_embedding=False</em>, <em class="sig-param">mask=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/transformer_encoder.html#TransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.transformer_encoder.TransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">EncoderBase.forward()</span></code></p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.transformer_encoder.TransformerEncoder.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opts</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">is_on_top=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/transformer_encoder.html#TransformerEncoder.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.transformer_encoder.TransformerEncoder.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.mean_encoder.MeanEncoder">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.mean_encoder.</code><code class="sig-name descname">MeanEncoder</code><span class="sig-paren">(</span><em class="sig-param">num_layers</em>, <em class="sig-param">embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/mean_encoder.html#MeanEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.mean_encoder.MeanEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mammoth.modules.encoder.EncoderBase" title="mammoth.modules.encoder.EncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.modules.encoder.EncoderBase</span></code></a></p>
<p>A trivial non-recurrent encoder. Simply applies mean pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_layers</strong> (<em>int</em>) – number of replicated layers</p></li>
<li><p><strong>embeddings</strong> (<a class="reference internal" href="#mammoth.modules.Embeddings" title="mammoth.modules.Embeddings"><em>mammoth.modules.Embeddings</em></a>) – embedding module to use</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.mean_encoder.MeanEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">lengths=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/mean_encoder.html#MeanEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.mean_encoder.MeanEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-func docutils literal notranslate"><span class="pre">EncoderBase.forward()</span></code></p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.mean_encoder.MeanEncoder.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opts</em>, <em class="sig-param">embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/mean_encoder.html#MeanEncoder.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.mean_encoder.MeanEncoder.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.layer_stack_encoder.LayerStackEncoder">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.layer_stack_encoder.</code><code class="sig-name descname">LayerStackEncoder</code><span class="sig-paren">(</span><em class="sig-param">embeddings</em>, <em class="sig-param">encoders</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_encoder.html#LayerStackEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_encoder.LayerStackEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mammoth.modules.encoder.EncoderBase" title="mammoth.modules.encoder.EncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.modules.encoder.EncoderBase</span></code></a></p>
<dl class="method">
<dt id="mammoth.modules.layer_stack_encoder.LayerStackEncoder.add_adapter">
<code class="sig-name descname">add_adapter</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_encoder.html#LayerStackEncoder.add_adapter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_encoder.LayerStackEncoder.add_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds the specified adapter with the name (adapter_group, sub_id)
into the module_id sharing group of the layer_stack_index’th stack</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_encoder.LayerStackEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">lengths=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_encoder.html#LayerStackEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_encoder.LayerStackEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>LongTensor</em>) – padded sequences of sparse indices <code class="docutils literal notranslate"><span class="pre">(src_len,</span> <span class="pre">batch,</span> <span class="pre">nfeat)</span></code></p></li>
<li><p><strong>lengths</strong> (<em>LongTensor</em>) – length of each sequence <code class="docutils literal notranslate"><span class="pre">(batch,)</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>final encoder state, used to initialize decoder</p></li>
<li><p>memory bank for attention, <code class="docutils literal notranslate"><span class="pre">(src_len,</span> <span class="pre">batch,</span> <span class="pre">hidden)</span></code></p></li>
<li><p>lengths</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor, FloatTensor, FloatTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_encoder.LayerStackEncoder.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opts</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">task_queue_manager</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_encoder.html#LayerStackEncoder.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_encoder.LayerStackEncoder.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor for use during training.</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_encoder.LayerStackEncoder.from_trans_opt">
<em class="property">classmethod </em><code class="sig-name descname">from_trans_opt</code><span class="sig-paren">(</span><em class="sig-param">model_opts</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">opt_stack</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_encoder.html#LayerStackEncoder.from_trans_opt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_encoder.LayerStackEncoder.from_trans_opt" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor for use during translation.</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_encoder.LayerStackEncoder.get_submodule">
<code class="sig-name descname">get_submodule</code><span class="sig-paren">(</span><em class="sig-param">layer_stack_index: int</em>, <em class="sig-param">module_id: str</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_encoder.html#LayerStackEncoder.get_submodule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_encoder.LayerStackEncoder.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
    path or resolves to something that is not an
    <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="decoders">
<h2>Decoders<a class="headerlink" href="#decoders" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mammoth.modules.decoder.DecoderBase">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.decoder.</code><code class="sig-name descname">DecoderBase</code><span class="sig-paren">(</span><em class="sig-param">attentional=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/decoder.html#DecoderBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.decoder.DecoderBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Abstract class for decoders.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>attentional</strong> (<em>bool</em>) – The decoder returns non-empty attention.</p>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.decoder.DecoderBase.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opts</em>, <em class="sig-param">embeddings</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/decoder.html#DecoderBase.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.decoder.DecoderBase.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor.</p>
<p>Subclasses should override this method.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.layer_stack_decoder.</code><code class="sig-name descname">LayerStackDecoder</code><span class="sig-paren">(</span><em class="sig-param">embeddings</em>, <em class="sig-param">decoders</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mammoth.modules.decoder.DecoderBase" title="mammoth.modules.decoder.DecoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.modules.decoder.DecoderBase</span></code></a></p>
<dl class="method">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder.add_adapter">
<code class="sig-name descname">add_adapter</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder.add_adapter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder.add_adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds the specified adapter with the name (adapter_group, sub_id)
into the module_id sharing group of the layer_stack_index’th stack</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tgt</em>, <em class="sig-param">memory_bank=None</em>, <em class="sig-param">step=None</em>, <em class="sig-param">memory_lengths=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opts</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">task_queue_manager</em>, <em class="sig-param">is_on_top=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor for use during training.</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder.from_trans_opt">
<em class="property">classmethod </em><code class="sig-name descname">from_trans_opt</code><span class="sig-paren">(</span><em class="sig-param">model_opts</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">opt_stack</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder.from_trans_opt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder.from_trans_opt" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternate constructor for use during translation.</p>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder.get_submodule">
<code class="sig-name descname">get_submodule</code><span class="sig-paren">(</span><em class="sig-param">layer_stack_index: int</em>, <em class="sig-param">module_id: str</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder.get_submodule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
    path or resolves to something that is not an
    <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.layer_stack_decoder.LayerStackDecoder.init_state">
<code class="sig-name descname">init_state</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">memory_bank</em>, <em class="sig-param">enc_hidden</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/layer_stack_decoder.html#LayerStackDecoder.init_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.layer_stack_decoder.LayerStackDecoder.init_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize decoder state.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.decoder_ensemble.EnsembleModel">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.decoder_ensemble.</code><code class="sig-name descname">EnsembleModel</code><span class="sig-paren">(</span><em class="sig-param">models</em>, <em class="sig-param">raw_probs=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/decoder_ensemble.html#EnsembleModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.decoder_ensemble.EnsembleModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.models.model.NMTModel</span></code></p>
<p>Dummy NMTModel wrapping individual real NMTModels.</p>
</dd></dl>

<dl class="class">
<dt id="mammoth.modules.transformer_decoder.TransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.transformer_decoder.</code><code class="sig-name descname">TransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">num_layers</em>, <em class="sig-param">d_model</em>, <em class="sig-param">heads</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">copy_attn</em>, <em class="sig-param">self_attn_type</em>, <em class="sig-param">dropout</em>, <em class="sig-param">attention_dropout</em>, <em class="sig-param">embeddings</em>, <em class="sig-param">max_relative_positions</em>, <em class="sig-param">aan_useffn</em>, <em class="sig-param">full_context_alignment</em>, <em class="sig-param">alignment_layer</em>, <em class="sig-param">alignment_heads</em>, <em class="sig-param">pos_ffn_activation_fn='relu'</em>, <em class="sig-param">layer_norm_module=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/transformer_decoder.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.transformer_decoder.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">mammoth.modules.transformer_decoder.TransformerDecoderBase</span></code></p>
<p>The Transformer decoder from “Attention is All You Need”.
<a class="bibtex reference internal" href="ref.html#dblp-journals-corr-vaswanispujgkp17" id="id3">[VSP+17]</a></p>
<div class="mermaid">
            graph BT
   A[input]
   B[multi-head self-attn]
   BB[multi-head src-attn]
   C[feed forward]
   O[output]
   A --&gt; B
   B --&gt; BB
   BB --&gt; C
   C --&gt; O
        </div><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_layers</strong> (<em>int</em>) – number of decoder layers.</p></li>
<li><p><strong>d_model</strong> (<em>int</em>) – size of the model</p></li>
<li><p><strong>heads</strong> (<em>int</em>) – number of heads</p></li>
<li><p><strong>d_ff</strong> (<em>int</em>) – size of the inner FF layer</p></li>
<li><p><strong>copy_attn</strong> (<em>bool</em>) – if using a separate copy attention</p></li>
<li><p><strong>self_attn_type</strong> (<em>str</em>) – type of self-attention scaled-dot, average</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout in residual, self-attn(dot) and feed-forward</p></li>
<li><p><strong>attention_dropout</strong> (<em>float</em>) – dropout in context_attn (and self-attn(avg))</p></li>
<li><p><strong>embeddings</strong> (<a class="reference internal" href="#mammoth.modules.Embeddings" title="mammoth.modules.Embeddings"><em>mammoth.modules.Embeddings</em></a>) – embeddings to use, should have positional encodings</p></li>
<li><p><strong>max_relative_positions</strong> (<em>int</em>) – Max distance between inputs in relative positions representations</p></li>
<li><p><strong>aan_useffn</strong> (<em>bool</em>) – Turn on the FFN layer in the AAN decoder</p></li>
<li><p><strong>full_context_alignment</strong> (<em>bool</em>) – whether enable an extra full context decoder forward for alignment</p></li>
<li><p><strong>alignment_layer</strong> (<em>int</em>) – N° Layer to supervise with for alignment guiding</p></li>
<li><p><strong>alignment_heads</strong> (<em>int</em>) – <ol class="upperalpha simple" start="14">
<li><p>of cross attention heads to use for alignment guiding</p></li>
</ol>
</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.transformer_decoder.TransformerDecoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tgt</em>, <em class="sig-param">memory_bank=None</em>, <em class="sig-param">step=None</em>, <em class="sig-param">memory_lengths=None</em>, <em class="sig-param">tgt_pad_mask=None</em>, <em class="sig-param">skip_embedding=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/transformer_decoder.html#TransformerDecoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.transformer_decoder.TransformerDecoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode, possibly stepwise.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sublayers">
<h2>Sublayers<a class="headerlink" href="#sublayers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="mammoth.modules.average_attn.AverageAttention">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.average_attn.</code><code class="sig-name descname">AverageAttention</code><span class="sig-paren">(</span><em class="sig-param">model_dim</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">aan_useffn=False</em>, <em class="sig-param">pos_ffn_activation_fn='relu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/average_attn.html#AverageAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.average_attn.AverageAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Average Attention module from
“Accelerating Neural Transformer via an Average Attention Network”
<a class="bibtex reference internal" href="ref.html#dblp-journals-corr-abs-1805-00631" id="id4">[ZXS18]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_dim</strong> (<em>int</em>) – the dimension of keys/values/queries,
must be divisible by head_count</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout parameter</p></li>
<li><p><strong>pos_ffn_activation_fn</strong> (<em>ActivationFunction</em>) – activation function choice for PositionwiseFeedForward layer</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.average_attn.AverageAttention.cumulative_average">
<code class="sig-name descname">cumulative_average</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">mask_or_step</em>, <em class="sig-param">layer_cache=None</em>, <em class="sig-param">step=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/average_attn.html#AverageAttention.cumulative_average"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.average_attn.AverageAttention.cumulative_average" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cumulative average as described in
<a class="bibtex reference internal" href="ref.html#dblp-journals-corr-abs-1805-00631" id="id5">[ZXS18]</a> – Equations (1) (5) (6)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>FloatTensor</em>) – sequence to average
<code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">dimension)</span></code></p></li>
<li><p><strong>mask_or_step</strong> – if cache is set, this is assumed
to be the current step of the
dynamic decoding. Otherwise, it is the mask matrix
used to compute the cumulative average.</p></li>
<li><p><strong>layer_cache</strong> – a dictionary containing the cumulative average
of the previous step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tensor of the same shape and type as <code class="docutils literal notranslate"><span class="pre">inputs</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.average_attn.AverageAttention.cumulative_average_mask">
<code class="sig-name descname">cumulative_average_mask</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">inputs_len</em>, <em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/average_attn.html#AverageAttention.cumulative_average_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.average_attn.AverageAttention.cumulative_average_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds the mask to compute the cumulative average as described in
<a class="bibtex reference internal" href="ref.html#dblp-journals-corr-abs-1805-00631" id="id6">[ZXS18]</a> – Figure 3</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – batch size</p></li>
<li><p><strong>inputs_len</strong> (<em>int</em>) – length of the inputs</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>A Tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">input_len)</span></code></p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mammoth.modules.average_attn.AverageAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">mask=None</em>, <em class="sig-param">layer_cache=None</em>, <em class="sig-param">step=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/average_attn.html#AverageAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.average_attn.AverageAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<em>FloatTensor</em>) – <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">model_dim)</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>gating_outputs <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">model_dim)</span></code></p></li>
<li><dl class="simple">
<dt>average_outputs average attention</dt><dd><p><code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">model_dim)</span></code></p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor, FloatTensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.multi_headed_attn.MultiHeadedAttention">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.multi_headed_attn.</code><code class="sig-name descname">MultiHeadedAttention</code><span class="sig-paren">(</span><em class="sig-param">head_count</em>, <em class="sig-param">model_dim</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">max_relative_positions=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/multi_headed_attn.html#MultiHeadedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.multi_headed_attn.MultiHeadedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Multi-Head Attention module from “Attention is All You Need”
<a class="bibtex reference internal" href="ref.html#dblp-journals-corr-vaswanispujgkp17" id="id7">[VSP+17]</a>.</p>
<p>Similar to standard <cite>dot</cite> attention but uses
multiple attention distributions simulataneously
to select relevant items.</p>
<div class="mermaid">
            graph BT
   A[key]
   B[value]
   C[query]
   O[output]
   subgraph Attn
     D[Attn 1]
     E[Attn 2]
     F[Attn N]
   end
   A --&gt; D
   C --&gt; D
   A --&gt; E
   C --&gt; E
   A --&gt; F
   C --&gt; F
   D --&gt; O
   E --&gt; O
   F --&gt; O
   B --&gt; O
        </div><p>Also includes several additional tricks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>head_count</strong> (<em>int</em>) – number of parallel heads</p></li>
<li><p><strong>model_dim</strong> (<em>int</em>) – the dimension of keys/values/queries,
must be divisible by head_count</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout parameter</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.multi_headed_attn.MultiHeadedAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">key</em>, <em class="sig-param">value</em>, <em class="sig-param">query</em>, <em class="sig-param">mask=None</em>, <em class="sig-param">layer_cache=None</em>, <em class="sig-param">attn_type=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/multi_headed_attn.html#MultiHeadedAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.multi_headed_attn.MultiHeadedAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the context vector and the attention vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>key</strong> (<em>FloatTensor</em>) – set of <cite>key_len</cite>
key vectors <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">key_len,</span> <span class="pre">dim)</span></code></p></li>
<li><p><strong>value</strong> (<em>FloatTensor</em>) – set of <cite>key_len</cite>
value vectors <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">key_len,</span> <span class="pre">dim)</span></code></p></li>
<li><p><strong>query</strong> (<em>FloatTensor</em>) – set of <cite>query_len</cite>
query vectors  <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">query_len,</span> <span class="pre">dim)</span></code></p></li>
<li><p><strong>mask</strong> – binary mask 1/0 indicating which keys have
zero / non-zero attention <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">query_len,</span> <span class="pre">key_len)</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>output context vectors <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">query_len,</span> <span class="pre">dim)</span></code></p></li>
<li><p>Attention vector in heads <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">head,</span> <span class="pre">query_len,</span> <span class="pre">key_len)</span></code>.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor, FloatTensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mammoth.modules.position_ffn.PositionwiseFeedForward">
<em class="property">class </em><code class="sig-prename descclassname">mammoth.modules.position_ffn.</code><code class="sig-name descname">PositionwiseFeedForward</code><span class="sig-paren">(</span><em class="sig-param">d_model</em>, <em class="sig-param">d_ff</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">activation_fn='relu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/position_ffn.html#PositionwiseFeedForward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.position_ffn.PositionwiseFeedForward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>A two-layer Feed-Forward-Network with residual layer norm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – the size of input for the first-layer of the FFN.</p></li>
<li><p><strong>d_ff</strong> (<em>int</em>) – the hidden layer size of the second-layer
of the FNN.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout probability in <span class="math notranslate nohighlight">\([0, 1)\)</span>.</p></li>
<li><p><strong>activation_fn</strong> (<em>ActivationFunction</em>) – activation function used.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="mammoth.modules.position_ffn.PositionwiseFeedForward.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mammoth/modules/position_ffn.html#PositionwiseFeedForward.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mammoth.modules.position_ffn.PositionwiseFeedForward.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Layer definition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> – <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">model_dim)</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">input_len,</span> <span class="pre">model_dim)</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mammoth.translation.html" class="btn btn-neutral float-right" title="Translation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="mammoth.html" class="btn btn-neutral float-left" title="Framework" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, HelsinkiNLP

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>